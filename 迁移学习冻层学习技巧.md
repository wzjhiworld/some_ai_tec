# 在迁移学习中的冻层训练

### 背景

在迁移学习的过程中，一般只是在基础模型的基础上，在后边添加几层进行训练，但是如何  
达到较好的泛化精度还是需要一些技巧。

### 分析

一般，我们认为训练好的基础模型，其越底层的层的参数提取的特征越具有通用性，所以如果  
在最开始训练模型的时候，就让全部的层都参与训练，会让底层的层的参数的通用性受到破坏  
给予这个假设，提出一个简单的训练方案：  

- 先冻结基础模型的参数，只训练后添加的几层
- 在后几层训练好以后，再解放前面冻结的基础模型的参数，全网进行训练

### 示例代码

**（代码是非完整的代码，基于keras，只是展示无法运行）**

~~~python
import os
import json
import yamnet as yamnet_model #yamnet的基础模型
import params
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau


yamnet = yamnet_model.yamnet_model()
yamnet.load_weights('./audioset/yamnet/yamnet.h5')

x = tf.keras.layers.Dense(64, activation='relu', name="transfer_1")(yamnet.layers[-3].output)
o = tf.keras.layers.Dropout(0.5, name="transfer_2")(x)
o = tf.keras.layers.Dense(20, activation='softmax', name="transfer_3")(o)

model = Model(inputs=yamnet.input, outputs=o)

#冻结基础层
for layer in model.layers:
    if "transfer" in layer.name:
        layer.trainable = True
        print(layer.name)
    else:
        layer.trainable = False

model.summary()

batch_size = 128

train_generator = DataGenerator(files_train,
                                labels,
                                batch_size=batch_size,
                                n_classes=20)
validation_generator = DataGenerator(files_val,
                                    labels,
                                    batch_size=batch_size,
                                    n_classes=20)

# Define training callbacks
checkpoint = ModelCheckpoint(model_out+'.h5',
                             monitor='val_loss', 
                             verbose=1,
                             save_best_only=True, 
                             mode='auto')

reducelr = ReduceLROnPlateau(monitor='val_loss', 
                              factor=0.5, 
                              patience=3, 
                              verbose=1)

# Compile model
optimizer = tf.keras.optimizers.Adam(lr=0.001)
model.compile(loss= tf.keras.losses.categorical_crossentropy, #'binary_crossentropy',
              optimizer=optimizer, metrics=["acc"])


model_history = model.fit(train_generator,
                            steps_per_epoch = len(train_generator),
                            epochs = 100,
                            validation_data = validation_generator,
                            validation_steps = len(validation_generator),
                            verbose = 1,
                            callbacks=[checkpoint, reducelr])

#放开基础层
for layer in model.layers:
    if "transfer" in layer.name:
        layer.trainable = True
        print(layer.name)
    else:
        layer.trainable = True

model_history = model.fit(train_generator,
                    steps_per_epoch = len(train_generator),
                    epochs = 100,
                    validation_data = validation_generator,
                    validation_steps = len(validation_generator),
                    verbose = 1,
                    callbacks=[checkpoint, reducelr])

~~~

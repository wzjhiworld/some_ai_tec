# 深度学习学习器的发展

过去学习深度学习的时候，经常不能很好的阐述目前深度学习优化器的发展，以及他们的优缺点和解决的问题。  
今天就来好好的写一写，也算是以输出倒逼自己学习。

> https://blog.csdn.net/racesu/article/details/106382892

目前主流的优化器主要为以下几种：

* SGD
* Momentnum
* NAG 
* AdaGrad 
* AdaDelta
* RMSprop
* Adam
* AdamW
* Lars
* Lamb

## SGD

SGD 全称为批随机梯度下降，以一个 Batch 的样本计算梯度更新模型参数。  
相关公式如下：  


# 深度学习学习器的发展

过去学习深度学习的时候，经常不能很好的阐述目前深度学习优化器的发展，以及他们的优缺点和解决的问题。  
今天就来好好的写一写，也算是以输出倒逼自己学习。

> https://blog.csdn.net/racesu/article/details/106382892  参考的博客
> https://www.cnblogs.com/hansjorn/p/12108282.html 优秀的理论分析博客

目前主流的优化器主要为以下几种：

* SGD
* Momentnum
* NAG 
* AdaGrad 
* RMSprop
* Adam
* AdamW
* Lars
* Lamb

## SGD

SGD 全称为批随机梯度下降，以一个 Batch 的样本计算梯度更新模型参数。  
相关公式如下：  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=&space;\theta&space;-&space;\eta&space;\cdot&space;{\nabla_{\theta}Loss}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=&space;\theta&space;-&space;\eta&space;\cdot&space;{\nabla_{\theta}Loss}" title="\theta = \theta - \eta \cdot {\nabla_{\theta}Loss}" /></a>

利用Loss 对权重参数 θ 求导，然后利用梯度进行参数 θ 的更新,其中 η 为学习率，用于控制学习过程的步长。  

SGD 随机梯度下降可能遇到的几个问题：

* 过小的学习率可能收敛过慢， 过大的学习率容易在峡谷形状或最优解附近震荡收敛困难， 所以一个合适的学习率是较难设置的。
* 在训练过程中调整学习率的化，那么一个合适的学习率改变计划也是一个难题，因为很难适应数据的特性。
* 所有的参数使用相同学习率是有问题的，如果数据分布很稀疏，所有的特征频率相差很大，需要以不同的程度更新所有的参数（类似于类不平衡问题），对于较少出现的特征需要使用更大的学习率。  
    > 想象这样一个场景，在一个分类网络的训练中，有一个Batch的数据中分别有四个对象分别为 [ 狗 狗 狗 猫 ] 因为每个样本回传的梯度的权重是一样，所以用于拟合狗的特征的参数其梯度会
    > 比较大，而用于拟合猫的特征的参数其梯度会很小，但是对网络的学习任务的重要性而言，每个参数的更新重要性应该是一致的，所以需要以不同的学习率来更新不同的网络参数有一定的合理性。
* 对于非凸平面的局部最小值和鞍点，鞍点的特点是周围的梯度都接近 0， 会导致梯度下降法局部失去效用，而无法找到更优解。

后续学界工业界为了解决或缓解上述问题，在 SGD 优化器的基础上提出了很多性能更优的优化器。

## Momentnum

Momentum 动量优化器在优化工程中，对于两种场景是具有优势的，分别如下：  

* 在梯度接近 0 的平坦位置，利用动量跳出平坦区域继续优化。
* 在遇到山谷等类型的 Loss 曲面时，可以通过动量的滤波优势，保持主要的前进方向，减少震荡。  

Momentum 的优化公式如下：

<a href="https://www.codecogs.com/eqnedit.php?latex=g_{t}&space;=&space;\gamma&space;g_{t-1}&space;&plus;&space;(1&space;-&space;\gamma)&space;\nabla_{\theta&space;}Loss" target="_blank"><img src="https://latex.codecogs.com/gif.latex?g_{t}&space;=&space;\gamma&space;g_{t-1}&space;&plus;&space;(1&space;-&space;\gamma)&space;\nabla_{\theta&space;}Loss" title="g_{t} = \gamma g_{t-1} + (1 - \gamma) \nabla_{\theta }Loss" /></a>  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\eta&space;\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\eta&space;\cdot&space;g_{t}" title="\theta_{t} = \theta_{t-1} - \eta \cdot g_{t}" /></a>

其中参数 γ 动量系数。  

简单来说动量优化器的设计采用了类似于移动平均的理念，认为每一次一个Batch计算的梯度中都包含了很多的噪音，通过滑动平均的方式来滤除噪音，减少震荡。  
下图为 SGD 与 Momentum 的效果比较： 

![image](https://user-images.githubusercontent.com/78289886/121833002-fc807000-ccfd-11eb-8849-2d3a89a45e37.png)

## NAG

如果将优化的过程看成一个小球在 Loss 曲面上的滚动下降，那么加入动量后的小球会盲目地跟从下坡的梯度，容易发生错误。例如在一个先下坡在上坡的 Loss 曲面上，
小球向下滚动到最低点的时候，由于还存在动量的影响，小球会继续向上坡的方向行进，从而产生振荡，影响学习的进程。为了使小球变得更聪明，当小球知道对面使上坡
的时候提前降低速度，而不是冲上另一个坡。NAG 就是为了应对动量学习器出现该问题提出的自适应的方法。

![image](https://user-images.githubusercontent.com/78289886/122141670-62950080-ce80-11eb-908c-1ae2716836bd.png)

NAG 学习过程公式：  

<a href="https://www.codecogs.com/eqnedit.php?latex=g_{t}&space;=&space;\gamma&space;g_{t-1}&space;&plus;&space;(1&space;-&space;\gamma)&space;\cdot&space;\nabla_{\theta}Loss(\theta&space;-&space;\mu&space;\gamma&space;g_{t-1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?g_{t}&space;=&space;\gamma&space;g_{t-1}&space;&plus;&space;(1&space;-&space;\gamma)&space;\cdot&space;\nabla_{\theta}Loss(\theta&space;-&space;\mu&space;\gamma&space;g_{t-1})" title="g_{t} = \gamma g_{t-1} + (1 - \gamma) \cdot \nabla_{\theta}Loss(\theta - \mu \gamma g_{t-1})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\eta&space;\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\eta&space;\cdot&space;g_{t}" title="\theta_{t} = \theta_{t-1} - \eta \cdot g_{t}" /></a>

NAG 利用 <a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;-&space;\mu&space;\cdot&space;\gamma&space;\cdot&space;g_{t-1}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;-&space;\mu&space;\cdot&space;\gamma&space;\cdot&space;g_{t-1}" title="\theta - \mu \cdot \gamma \cdot g_{t-1}" /></a> 作为下个位置的估计值，而不是在当前位置计算梯度，这样就可以通过未来的位置信息来调整梯度，防止上述所出现的问题。

**问题**：从公式上看，NAG的计算过程需要先模拟未来位置的梯度，这在代码处理和梯度计算方面的消耗都要高于其他的一些同类优化器，可能就是目前很少见到使用的原因吧（猜测）。

## AdaGrad

> 核心思想：对频繁出现的特征执行更小的参数更新（低学习率），对于出现较少的特征予以更大的参数更新。所以该方法非常适用于处理稀疏数据，而且很大程度上提高了SGD的鲁棒性。

对于优化问题来说，随着梯度的增大，我们的步长也在增大， 在 AdaGrad 中随着梯度的增大， 分母增大，学习步长在减少，所以在优化的初始阶段，由于参数距离最优解比较远，所以  
需要大的步长，而随着训练的进行，参数开始接近最优解，这时候需要用小的步长微调。类似于学习率衰减的过程。

AdaGrad 的公式：  

<a href="https://www.codecogs.com/eqnedit.php?latex=\sigma&space;^{t}&space;=&space;\sqrt{\frac{1}{t&space;&plus;&space;1}\cdot&space;\sum_{i=0}^{t}&space;(g_{i})^{2}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\sigma&space;^{t}&space;=&space;\sqrt{\frac{1}{t&space;&plus;&space;1}\cdot&space;\sum_{i=0}^{t}&space;(g_{i})^{2}}" title="\sigma ^{t} = \sqrt{\frac{1}{t + 1}\cdot \sum_{i=0}^{t} (g_{i})^{2}}" /></a>  

<a href="https://www.codecogs.com/eqnedit.php?latex=\eta&space;^{t}&space;=&space;\frac{\eta&space;}{\sqrt{t&space;-&space;1}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\eta&space;^{t}&space;=&space;\frac{\eta&space;}{\sqrt{t&space;+&space;1}}" title="\eta ^{t} = \frac{\eta }{\sqrt{t + 1}}" /></a>  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;^{t}&space;=&space;\theta&space;^{t-1}&space;-&space;\frac{\eta&space;^{t}}{\sigma&space;^{t}}\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;^{t}&space;=&space;\theta&space;^{t-1}&space;-&space;\frac{\eta&space;^{t}}{\sigma&space;^{t}}\cdot&space;g_{t}" title="\theta ^{t} = \theta ^{t-1} - \frac{\eta ^{t}}{\sigma ^{t}}\cdot g_{t}" /></a>

**final:**  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;^{t}&space;=&space;\theta&space;^{t-1}&space;-&space;\frac{\eta&space;}{\sqrt{\sum_{i=1}^{t}&space;(g_{i})^{2}&space;&plus;&space;\epsilon&space;}}\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;^{t}&space;=&space;\theta&space;^{t-1}&space;-&space;\frac{\eta&space;}{\sqrt{\sum_{i=1}^{t}&space;(g_{i})^{2}&space;&plus;&space;\epsilon&space;}}\cdot&space;g_{t}" title="\theta ^{t} = \theta ^{t-1} - \frac{\eta }{\sqrt{\sum_{i=1}^{t} (g_{i})^{2} + \epsilon }}\cdot g_{t}" /></a>  

**为什么要除以梯度的平方和?**  

首先要知道 AdaGrad 其实是对于梯度较大的参数，用以较小的学习率，而对于梯度较小的参数，用以较大的学习率。这里面蕴含了一些牛顿迭代法的思想。  
利用梯度个平方和来模拟 Loss 的二阶梯度，因为直接计算 Loss 的二阶导数比较耗时。

**直观的理解**

在Loss空间中，较平缓的区域梯度值低需要较大的学习步长，而较陡的区域，如果使用同样的学习率，容易产生振荡，所以需要较小的学习步长来调整，这样
可以加快训练的过程。

**缺点**

* 从公式中可以看出，计算累积梯度平方和的时候，累积梯度平方和会越来越大，导致实际的学习率越来越小，阻碍后期的收敛
* 同上分析，要让训练过程得到一个合理的结果，必须要增对训练任务审计一个合适的全局初始化速率。

后续诞生的学习器弥补了 AdaGrad 的一些缺点。

## RMSProb

RMSProb 学习器用于解决 AdaGrad 由于梯度平方和累积一直变大导致学习步长持续下降的问题。所以在RMSProb中利用移动平均的方式计算累积的梯度平方  
和，这样就有效解决了梯度平方和累积得越来越大的问题。

**计算公式：**

<a href="https://www.codecogs.com/eqnedit.php?latex=\widetilde{g_{t}}&space;=&space;\lambda&space;\cdot&space;\widetilde{g}_{t-1}&space;&plus;&space;(1&space;-&space;\lambda)\cdot&space;g_{t}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\widetilde{g_{t}}&space;=&space;\lambda&space;\cdot&space;\widetilde{g}_{t-1}&space;&plus;&space;(1&space;-&space;\lambda)\cdot&space;g_{t}^{2}" title="\widetilde{g_{t}} = \lambda \cdot \widetilde{g}_{t-1} + (1 - \lambda)\cdot g_{t}^{2}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;_{t}&space;=&space;\theta&space;_{t-1}&space;-&space;\frac{\eta&space;}{\sqrt{\widetilde{g_{t}}&space;&plus;&space;\epsilon}}\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;_{t}&space;=&space;\theta&space;_{t-1}&space;-&space;\frac{\eta&space;}{\sqrt{\widetilde{g_{t}}&space;&plus;&space;\epsilon}}\cdot&space;g_{t}" title="\theta _{t} = \theta _{t-1} - \frac{\eta }{\sqrt{\widetilde{g_{t}} + \epsilon}}\cdot g_{t}" /></a>

## Adam

在深度学习优化器的发展中，诞生了以动量思想和自适应步长两大思想的优化器设计，有没有可以兼容两种思路优点的学习器呢？  
Adam 就应允而生了。

**公式**

<a href="https://www.codecogs.com/eqnedit.php?latex=m_{t}&space;=&space;\beta_{1}&space;\cdot&space;m_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{1})\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m_{t}&space;=&space;\beta_{1}&space;\cdot&space;m_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{1})\cdot&space;g_{t}" title="m_{t} = \beta_{1} \cdot m_{t - 1} + (1 - \beta_{1})\cdot g_{t}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=v_{t}&space;=&space;\beta_{2}&space;\cdot&space;v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{2})\cdot&space;g_{t}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{t}&space;=&space;\beta_{2}&space;\cdot&space;v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{2})\cdot&space;g_{t}^{2}" title="v_{t} = \beta_{2} \cdot v_{t - 1} + (1 - \beta_{2})\cdot g_{t}^{2}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{m_{t}}&space;=&space;\frac{m_{t}}{1&space;-&space;\beta_{1}^{t}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{m_{t}}&space;=&space;\frac{m_{t}}{1&space;-&space;\beta_{1}^{t}}" title="\hat{m_{t}} = \frac{m_{t}}{1 - \beta_{1}^{t}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{v_{t}}&space;=&space;\frac{v_{t}}{1&space;-&space;\beta_{2}^{t}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{v_{t}}&space;=&space;\frac{v_{t}}{1&space;-&space;\beta_{2}^{t}}" title="\hat{v_{t}} = \frac{v_{t}}{1 - \beta_{2}^{t}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t}&space;=&space;\theta_{t&space;-&space;1}&space;-&space;\frac{\eta&space;}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}\cdot&space;\hat{m_{t}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t}&space;=&space;\theta_{t&space;-&space;1}&space;-&space;\frac{\eta&space;}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}\cdot&space;\hat{m_{t}}" title="\theta_{t} = \theta_{t - 1} - \frac{\eta }{\sqrt{\hat{v_{t}} + \epsilon }}\cdot \hat{m_{t}}" /></a>

**Adam中的偏差矫正问题**

在上述的参数更新公式中，<a href="https://www.codecogs.com/eqnedit.php?latex=m_{0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m_{0}" title="m_{0}" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=v_{0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{0}" title="v_{0}" /></a>的初始值都是0，这导致在计算的过程中，在优化的前几步，计算的值都严重的偏向<a href="https://www.codecogs.com/eqnedit.php?latex=m_{0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m_{0}" title="m_{0}" /></a>和<a href="https://www.codecogs.com/eqnedit.php?latex=v_{0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{0}" title="v_{0}" /></a>，这使得训练的前几步变得不稳定，所以才有了后面的偏差矫正的过程。  

Adam 是一个综合性能极优的优化器。

## AdamW

**优化初衷**

由于 Adam 优化器在与 L2 正则配合的时候，会导致 L2 正则一定程度上失效。原因是L2 正则在梯度优化的过程中，担任的角色是让梯度的更新一直减去一个 正则系数乘以当前权重值 的更新量  
这有使权重参数最终偏小的趋势，使模型不容易过拟合。但是 Adam 优化公式中的更新值  
<a href="https://www.codecogs.com/eqnedit.php?latex=\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}" title="\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}} + \epsilon }}" /></a>  
形式上类似于对梯度做了模长归一化，这就导致L2 正则的作用倾向于消失，因为对于 L2 正则在优化过程中的作用而言，其希望较大的权重参数在优化的过程中能减去一个与其成比列的较大的值而使最终的权重系数偏小，而 Adam 优化公式中的类似于模长归一化的作用使其作用失效。

**优化方法**

AdamW的优化方式也极其简单，就是在原有的基础上，将权重参数更新公式中的权重衰减项显式的加回。

**公式**

<a href="https://www.codecogs.com/eqnedit.php?latex=m_{t}&space;=&space;\beta_{1}&space;\cdot&space;m_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{1})\cdot&space;g_{t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?m_{t}&space;=&space;\beta_{1}&space;\cdot&space;m_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{1})\cdot&space;g_{t}" title="m_{t} = \beta_{1} \cdot m_{t - 1} + (1 - \beta_{1})\cdot g_{t}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=v_{t}&space;=&space;\beta_{2}&space;\cdot&space;v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{2})\cdot&space;g_{t}^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?v_{t}&space;=&space;\beta_{2}&space;\cdot&space;v_{t&space;-&space;1}&space;&plus;&space;(1&space;-&space;\beta_{2})\cdot&space;g_{t}^{2}" title="v_{t} = \beta_{2} \cdot v_{t - 1} + (1 - \beta_{2})\cdot g_{t}^{2}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{m_{t}}&space;=&space;\frac{m_{t}}{1&space;-&space;\beta_{1}^{t}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{m_{t}}&space;=&space;\frac{m_{t}}{1&space;-&space;\beta_{1}^{t}}" title="\hat{m_{t}} = \frac{m_{t}}{1 - \beta_{1}^{t}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\hat{v_{t}}&space;=&space;\frac{v_{t}}{1&space;-&space;\beta_{2}^{t}}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\hat{v_{t}}&space;=&space;\frac{v_{t}}{1&space;-&space;\beta_{2}^{t}}" title="\hat{v_{t}} = \frac{v_{t}}{1 - \beta_{2}^{t}}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t}&space;=&space;\theta_{t&space;-&space;1}&space;-&space;\eta&space;\cdot&space;(\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}&space;&plus;&space;\lambda&space;\cdot&space;\theta_{t-1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t}&space;=&space;\theta_{t&space;-&space;1}&space;-&space;\eta&space;\cdot&space;(\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}&space;&plus;&space;\epsilon&space;}}&space;&plus;&space;\lambda&space;\cdot&space;\theta_{t-1})" title="\theta_{t} = \theta_{t - 1} - \eta \cdot (\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}} + \epsilon }} + \lambda \cdot \theta_{t-1})" /></a>

## Lars

Lars 优化器是专门为分布式下大 Batch_Size 优化过程而设计的优化器。  

> 参考应用的博客 https://www.jianshu.com/p/e430620d3acf

**问题**

随着深度学习的发展，为了提升训练的速度，越来越依赖在多个GPU上进行大的 Batch Size 训练。但是大的Batch Size 如何寻找与其配套的学习率是一个难题，而且大的Batch Size训练容易陷入一些尖锐的  
极小值解，导致模型最终的泛化能力不佳。
但是当训练周期数（epoches）不变时，增大batch size将减少网络权重的更新次数，因为一次 epoch 对应的一个 batch 训练的次数减少了。 为了弥补该问题，很多研究者建议当batch size增加k倍时，
将学习率也增加k倍，但是当batch size很大的时候，学习率增加太大会导致学习不稳定，尤其是在训练早期阶段，如果学习率太大，则参数可能在错误的方向上更新很多（就好比人生在最开始就选择了错误的方向，  
可能后续就无法纠偏了，必然导致最终成就的降低），从而导致模型最终的表现很差。另外，学习率太大的话也容易导致训练过程不收敛。

**解决方案**

之前主流的用来缓解该问题的方法是学习率warm-up，在训练的前几个周期，从一个比较小的学习率开始，线性增加到最终使用的学习率（也就是k倍增加后的学习率）。Lars的论文从实验观察出发，提出了可以替代warm-up 的方法——分层自适应学习率缩放(Layer-wise Adaptive Rate Scaling)，从一个新颖的方向缓解学习率过大的问题。

**公式**

<a href="https://www.codecogs.com/eqnedit.php?latex=\lambda^{l}&space;=&space;\eta&space;\cdot&space;\frac{\left&space;\|&space;\theta^{l}&space;\right&space;\|}{\left&space;\|&space;\nabla_{\theta}L(\theta^{l})&space;\right&space;\|}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\lambda^{l}&space;=&space;\eta&space;\cdot&space;\frac{\left&space;\|&space;\theta^{l}&space;\right&space;\|}{\left&space;\|&space;\nabla_{\theta}L(\theta^{l})&space;\right&space;\|}" title="\lambda^{l} = \eta \cdot \frac{\left \| \theta^{l} \right \|}{\left \| \nabla_{\theta}L(\theta^{l}) \right \|}" /></a>  

上式为当前层的局部学习率。
权重更新公式为如下：  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\gamma&space;\cdot&space;\lambda^{l}\cdot&space;\nabla_{\theta}L(\theta^{l})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta_{t}&space;=&space;\theta_{t-1}&space;-&space;\gamma&space;\cdot&space;\lambda^{l}\cdot&space;\nabla_{\theta}L(\theta^{l})" title="\theta_{t} = \theta_{t-1} - \gamma \cdot \lambda^{l}\cdot \nabla_{\theta}L(\theta^{l})" /></a>

## Lamb

**问题**

Lars 优化器在 imagenet 图片分类问题上表现优异，但是在Bert的表现上较为差强人意，为了弥补 Lars的缺点，提出了结合了 Adam 理念的 Lamb 学习器。

**公式**

![image](https://user-images.githubusercontent.com/78289886/123818813-e1cc0f00-d92b-11eb-8272-65d710410766.png)













# 深度学习学习器的发展

过去学习深度学习的时候，经常不能很好的阐述目前深度学习优化器的发展，以及他们的优缺点和解决的问题。  
今天就来好好的写一写，也算是以输出倒逼自己学习。

> https://blog.csdn.net/racesu/article/details/106382892  参考的博客

目前主流的优化器主要为以下几种：

* SGD
* Momentnum
* NAG 
* AdaGrad 
* AdaDelta
* RMSprop
* Adam
* AdamW
* Lars
* Lamb

## SGD

SGD 全称为批随机梯度下降，以一个 Batch 的样本计算梯度更新模型参数。  
相关公式如下：  

<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=&space;\theta&space;-&space;\eta&space;\cdot&space;{\nabla_{\theta}Loss}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=&space;\theta&space;-&space;\eta&space;\cdot&space;{\nabla_{\theta}Loss}" title="\theta = \theta - \eta \cdot {\nabla_{\theta}Loss}" /></a>

利用Loss 对权重参数 θ 求导，然后利用梯度进行参数 θ 的更新,其中 η 为学习率，用于控制学习过程的步长。  

SGD 随机梯度下降可能遇到的几个问题：

* 过小的学习率可能收敛过慢， 过大的学习率容易在峡谷形状或最优解附近震荡收敛困难， 所以一个合适的学习率是较难设置的。
* 在训练过程中调整学习率的化，那么一个合适的学习率改变计划也是一个难题，因为很难适应数据的特性。
* 所有的参数使用相同学习率是有问题的，如果数据分布很稀疏，所有的特征频率相差很大，需要以不同的程度更新所有的参数（类似于类不平衡问题），对于较少出现的特征需要使用更大的学习率。  
    > 想象这样一个场景，在一个分类网络的训练中，有一个Batch的数据中分别有四个对象分别为 [ 狗 狗 狗 猫 ] 因为每个样本回传的梯度的权重是一样，所以用于拟合狗的特征的参数其梯度会
    > 比较大，而用于拟合猫的特征的参数其梯度会很小，但是对网络的学习任务的重要性而言，每个参数的更新重要性应该是一致的，所以需要以不同的学习率来更新不同的网络参数有一定的合理性。
* 对于非凸平面的局部最小值和鞍点，鞍点的特点是周围的梯度都接近 0， 会导致梯度下降法局部失去效用，而无法找到更优解。

后续学界工业界为了解决或缓解上述问题，在 SGD 优化器的基础上提出了很多性能更优的优化器。

## Momentnum
